{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPk5zngxSOUD/HlNSCgmPkT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/megumihoshino/Basic-Java-Script-/blob/main/ML_modelpractice1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QPO81rf95Pi",
        "outputId": "c7498ca6-80c5-4825-8347-a74058e0978f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 22s 11ms/step - loss: 0.4738 - accuracy: 0.8316\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.3588 - accuracy: 0.8683\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.3231 - accuracy: 0.8802\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 18s 9ms/step - loss: 0.2992 - accuracy: 0.8889\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.2795 - accuracy: 0.8954\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.2644 - accuracy: 0.9016\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.2545 - accuracy: 0.9054\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.2415 - accuracy: 0.9100\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.2306 - accuracy: 0.9129\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 18s 9ms/step - loss: 0.2246 - accuracy: 0.9147\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79c993dcd330>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#implementation tensorflow and keras\n",
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist #dataset fashion mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0 #\n",
        "\n",
        "model  = tf.keras.models.Sequential([ #buat arsitektur MLP, definisi layer2\n",
        "    tf.keras.layers.Flatten(input_shape = (28,28)), #input\n",
        "    tf.keras.layers.Dense(512, activation = tf.nn.relu), #hidden\n",
        "    tf.keras.layers.Dense(10, activation = tf.nn.softmax) #ouput/dense layer\n",
        "])\n",
        "\n",
        "model.compile(optimizer = tf.optimizers.Adam(),#optimizer\n",
        "              loss = 'sparse_categorical_crossentropy', #klasifikasi 3 kls atau lbh\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs = 10)  #jml brp kli model kita melakukan back propagation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PRA PEMROSESAN DATA UTK MODEL\n",
        "'''\n",
        "1. ubah dataset ke dlm btk larik/array\n",
        "2. pisah atribut dan label pd data\n",
        "3. lakukan normalisasi (mengubah skala data mjd skala yg seragam)\n",
        "4. lakukan train data dan test data\n",
        "'''\n",
        "\n",
        "#PEMROSESAN DATA GBR\n",
        "'''\n",
        "- utk versi TF <=2.9, bisa lakukan augmentasi menggunakan fungsi\n",
        "ImageDataGenerator utk data latih dan data validasi.\n",
        "- utk versi TF >2.9, bisa menggunakan tf.data dan layer augmentasi\n",
        "scr lngsng pd layer sequentials\n",
        "'''\n",
        "\n",
        "#data augmentasi, TF <=2.9\n",
        "'''\n",
        "- ImageDataGenerator = fungsi utk mempersiapkan data latih dan data\n",
        "validasi\n",
        "- Augmentasi gbr = teknik utk memperbyk data latih dg cara duplikasi\n",
        "gbr yg telah ada (rescale, rotation, shear, zoom, dst)\n",
        "'''\n"
      ],
      "metadata": {
        "id": "ziTg4kpmOQq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kode augmentasi gambar, TF <=2.9\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    rotation_range = 40,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    fill_mode = 'nearest',\n",
        "    horizontal_flip = 'True')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "metadata": {
        "id": "Zn4iAIQ2QAqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KODE AUGMENTASI JIKA TF>2.9\n",
        "\n",
        "#mengubah ukuran gbr dan pixel scr eksplisit\n",
        "IMG_size = 180\n",
        "\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "    layers.Resizing(IMG_size, IMG_size),\n",
        "    layers.Rescaling(1.0/255)\n",
        "])\n",
        "\n",
        "#fungsi augmentasi\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "#menerapkan data augmentasi\n",
        "'''\n",
        "pd proses ini kita memasukkan data augmentasi tsb pd layer sequentials\n",
        "ketika membangung struktur neural network agar seluruh dataset\n",
        "yg ada dpt diproses dan bs melakukan pelatihan dg baik.\n",
        "'''\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    #menambahkan processing image yg telah didefinisikan\n",
        "    resize_and_rescale,\n",
        "    data_augmentation,\n",
        "    layers.Conv2D(16,3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    #kita menyesuaikan sisa layer dg kasus yg dimiliki\n",
        "])\n"
      ],
      "metadata": {
        "id": "UnkLxyAfUTk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HAL PENTING DLM MEMBANGUN MODEL NEURAL NETWORK DG TF > 2.9\n",
        "'''\n",
        "1. device\n",
        "  data augmentasi akn berjln pd device yg digunakan, baik cloud\n",
        "  environment maupun lokal. proses ini akn berjln scr bersamaan\n",
        "  dg eksekusi layer lainnya. hal ini menyebabkan komputasi yg dilakukan\n",
        "  akn lbh berat shg penggunaan GPU akn lbh menguntungkan.\n",
        "\n",
        "2. ekspor model\n",
        "  ketika mengekspor model menggunakan model.save, layer processing\n",
        "  akn disimpan bersm dg layer lainnya. jika menggunakan model ini,\n",
        "  maka akn scr otomatis menstandarkan gbr (sesuai konfigurasi layer)\n",
        "  hal ini dpt membantu utk implementasi ulang logika tsb di sisi server.\n",
        "\n",
        "3. data augmentasi tdk akn aktif pd saat pengujian shg gbr input\n",
        "  akn ditmbh slm pemanggilan model.fit (bkn model.evaluate atau\n",
        "  model.predict)\n",
        "'''"
      ],
      "metadata": {
        "id": "T_1N9slUqZ9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PEMROSESAN DATA BAHASA\n",
        "#CTH KODE\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\"I LOVE MY HUSBANDO TSUKISHIMA KEI\"]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "#tokenizer berfungsi utk membangun indeks kata dari korpus kata\n",
        "\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "#text_to_sequences utk konversi teks mjd urutan numerik berdsrkan kamus yg dibuat\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFpbkhhFsoxA",
        "outputId": "9b46814a-b3a1-4a07-a59a-b35b6eadb5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'husbando': 4, 'tsukishima': 5, 'kei': 6}\n",
            "[[1, 2, 3, 4, 5, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PEMROSESAN DATA BAHAS 2.0\n",
        "#CTH KODE\n",
        "'''\n",
        "dlm pemrosesan data bhs kita menggunakan fungsi texts_to_sequences yg berguna\n",
        "utk mengonversi teks mjd urutan indeks numerk berdsrkan kamus yg dibuat.\n",
        "\n",
        "namun, kondisi tsb msh ringkih dan performanya blm ckp baik krn btk dari data yg dimiliki\n",
        "msh beragam dan terbatas pd kata2 yg tdk termasuk dlm dataset. hal tsb bisa diatasi\n",
        "dg fungsi oov_token dan padding shg dataset memiliki shape yg serupa.\n",
        "\n",
        "- pd kasus ini, oov_token berguna utk mengatasi kata yg tdk termsk pd tokenizer shg akn\n",
        "diubah mjd spesial value yg bs kita tentukan sendiri.\n",
        "'''\n"
      ],
      "metadata": {
        "id": "OnVxIEGTwLr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CTH KODE penggunaan oov_token dan texts_to_sequences serta padding\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "sentences = [\"Tsukishima Kei\",\n",
        "              \"A smart middle blocker from Karasuno.\"]\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sentences = [\"Tsukishima Kei\",\n",
        "             \"A smart middle blocker from Karasuno.\",\n",
        "             \"Kei is Karasuno's volleyball member\",\n",
        "             \"Do you think he is cute? Isn't he?\"]\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "padded = pad_sequences(sequences, maxlen = 10, truncating = \"post\", padding = \"post\" )\n",
        "\n",
        "print(\"Tokenizer: \", tokenizer.word_index)\n",
        "print(\"Sequences:\", sequences)\n",
        "print(\"Padded: \", padded)\n",
        "\n",
        "#dalam cth kode di atas, pada kata yg tidak termasuk akan diberi nilai \"1\" dalam hal ini akan diproses dengan kata \"<OOV>\""
      ],
      "metadata": {
        "id": "ny4Rg6G07Jw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cdad139-6af2-41b7-9957-d67f78314a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer:  {'<OOV>': 1, 'tsukishima': 2, 'kei': 3, 'a': 4, 'smart': 5, 'middle': 6, 'blocker': 7, 'from': 8, 'karasuno': 9}\n",
            "Sequences: [[2, 3], [4, 5, 6, 7, 8, 9], [3, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]\n",
            "Padded:  [[2 3 0 0 0 0 0 0 0 0]\n",
            " [4 5 6 7 8 9 0 0 0 0]\n",
            " [3 1 1 1 1 0 0 0 0 0]\n",
            " [1 1 1 1 1 1 1 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MENGGUNAKAN MODEL UTK MELAKUKAN PREDIKSI\n",
        "\n",
        "'''\n",
        "kita bisa menggunakan API Keras utk membuat model yg sudah dibuat\n",
        "membuat program ML prediksi dg mudah dan cepat\n",
        "'''\n",
        "#CTH KODE\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "#merubah angka2 mjd sebuah larik/array agar mudah diterima oleh Keras\n",
        "\n",
        "x = np.array([-3.5, -2.5, -1.5, -1, 0, 1.5, 2.5, 3.5, 4.5, 5.0], dtype = float)\n",
        "y = np.array([6.0, 7.0, 8.0, 9.0, 9.0, 10.0, 11.0, 12.0], dtype = float)\n",
        "\n",
        "#buat model neural network  dg memanggil fungsi tf.keras.Sequential()\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layer.Dense(units = 1, input_shape = [1])\n",
        "])"
      ],
      "metadata": {
        "id": "ghtSfG8k_xTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "parameter?\n",
        "pada cth kode di atas, terdpt parameter units dan input_shape.\n",
        "- parameter units:\n",
        "  parameter units dari fungsi tf.keras.Dense() adlh jlm perceptron yg dimiliki\n",
        "  olh layer tsb. yg perlu diperhatikan, mdodel sequential adlh layer pertama dr model\n",
        "  tsb adlh hrs memiliki parameter input_shape agar model bs mengenali btk input yg akn\n",
        "  diproses.\n",
        "\n",
        "  - parameter input_shape\n",
        "    parameter ini adlh btk dr tiap elemen input yg akn diterima dr model. pd kasus ini\n",
        "    tiap elemen dr data adlh sebuah bil. numerik 1 digit, shg input_shape dpt\n",
        "    diisi dg angka 1. jika sebuah elemen dr datset kita berupa gbr yg memiliki\n",
        "    dimensi 32*32 pixel, maka input_shape dpt diisi dg angka [32*32]\n",
        "'''"
      ],
      "metadata": {
        "id": "B59XVAYeFY28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "fungsi kerugian (loss function)\n",
        "- adalah sebuah metrik yg digunakan dlm machne learning utk evaluasi sbrp\n",
        "baik sebuah model utk prediksi target pd data training.\n",
        "bbrp cth loss function yaitu:\n",
        "- mean_squared_error -> utk mslh regresi\n",
        "- binary_crossentropy -> utk klasifikasi biner\n",
        "- categorical_crossentropy -> utk klasifikasi multi kelas\n",
        "\n",
        "utk implementasi optimizer dan loss function pke TF, bs pakai fungsi ini:\n",
        "model.compile()\n",
        "'''\n",
        "#cth\n",
        "model.compile(optimizer = \"sgd\", loss = \"mean_squared_error\")\n",
        "\n",
        "#fungsi fit dlm ML\n",
        "'''\n",
        "fungsi fit dlm machine learning berguna agar model dpt mempelajari\n",
        "hubungan antara atribut dan label pd dataset.\n",
        "\n",
        "epochs adalah banyaknya yg dibutuhkan oleh sebuah model NN hrs belajar\n",
        "utk memperbaiki akurasinya.'''\n",
        "\n",
        "#cth fungsi epochs\n",
        "model.fit(x,y, epochs = 500)\n",
        "\n",
        "'''\n",
        "utk bbrp kasus nilai epochs yg bsr belum tentu meningkatkan nilai akurasi.\n",
        "bs saja loss yg didpt stagnan atau malah semakin besaar. oleh krn itu\n",
        "kita perlu melakukan trial dan eror utk memperhatikan loss yg didpt ketika\n",
        "melakukan pelatihan.'''\n"
      ],
      "metadata": {
        "id": "z6uJbDrvHbn_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}